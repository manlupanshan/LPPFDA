%%% Update when I have time
import torch
import torch.nn as nn
from torchvision import models
import torch.nn.functional as F

#1. Code for the PSF Attention Layer (also include the baseline)
class ForwardAttentionLayer(nn.Module):
    def __init__(self, inputChannels, outputChannels, kernelSize, stride, padding, dilation=1, groups=1, bias=False):
        super(ForwardAttentionLayer, self).__init__()

        self.conv = nn.Conv2d(inputChannels, outputChannels, kernelSize, stride, padding, dilation, groups, bias)

        dim = inputChannels
        layer_scale_init_value = 1e-6

        self.dwconv = nn.Conv2d(dim, dim, kernel_size=4, stride=2, padding=1, groups=dim)  # depthwise conv
        self.norm = LayerNorm(dim, eps=1e-6, data_format="channels_last")
        self.pwconv1 = nn.Linear(dim, 4 * dim)  # pointwise/1x1 convs, implemented with linear layers
        self.act = nn.GELU()
        self.pwconv2 = nn.Linear(4 * dim, outputChannels)
        self.gamma = nn.Parameter(layer_scale_init_value * torch.ones((outputChannels,)),
                                  requires_grad=True) if layer_scale_init_value > 0 else None

        if inputChannels == 4:
            self.maskConv = nn.Conv2d(3, outputChannels, kernelSize, stride, padding, dilation, \
                groups, bias)
            self.dwconv_mask = nn.Conv2d(3, dim, kernel_size=4, stride=2, padding=1)

        else:
            self.maskConv = nn.Conv2d(inputChannels, outputChannels, kernelSize, stride, padding, \
                dilation, groups, bias)
            self.dwconv_mask = nn.Conv2d(dim, dim, kernel_size=4, stride=2, padding=1, groups=dim)

        self.norm_mask = LayerNorm(dim, eps=1e-6, data_format="channels_last")
        self.pwconv1_mask = nn.Linear(dim, 2 * dim)  # pointwise/1x1 convs, implemented with linear layers
        self.pwconv2_mask = nn.Linear(2 * dim, outputChannels)

        self.conv1 = nn.Conv2d(outputChannels, outputChannels, kernel_size=1, bias=False)
        # self.conv3 = nn.Conv2d(outputChannels, outputChannels, kernel_size=3, padding=1, bias=False)  

        self.conv.apply(weights_init())
        self.maskConv.apply(weights_init())
        # self.conv1.apply(weights_init())
        # self.conv3.apply(weights_init())
        self.activationFuncG_A = GaussActivation(1.1, 2.0, 1.0, 1.0)
        self.updateMask = MaskUpdate(0.8) # 

    def forward(self, inputFeatures, inputMasks):
        convFeatures = self.conv(inputFeatures)
        shortcut = convFeatures
        # convOut = convFeatures

        x = self.dwconv(inputFeatures)
        x = x.permute(0, 2, 3, 1)  # [N, C, H, W] -> [N, H, W, C]
        x = self.norm(x)
        x = self.pwconv1(x)
        x = self.act(x)
        x = self.pwconv2(x)
        if self.gamma is not None:
            x = self.gamma * x
        x = x.permute(0, 3, 1, 2)  # [N, H, W, C] -> [N, C, H, W]
        # print(x.shape)
        x = shortcut + x
        convOut = x
        convFeatures = x

        maskFeatures = self.maskConv(inputMasks)
        shortcut_mask = maskFeatures

        y = self.dwconv_mask(inputMasks)
        y = y.permute(0, 2, 3, 1)
        y = self.pwconv1_mask(y)
        y = self.pwconv2_mask(y)
        y = y.permute(0, 3, 1, 2)  # [N, H, W, C] -> [N, C, H, W]
        maskFeatures = shortcut_mask + y  

        ## Use convolution to extract and update the PSF feature (commonly referred to as the mask feature in attention modules) or not. If you want to update this ,delete the'#'.
        # y = self.maskConv(inputMasks)
        # y = y.permute(0, 2, 3, 1)
        # y = self.pwconv1_mask(y)
        # y = self.pwconv2_mask(y)
        # y = y.permute(0, 3, 1, 2)
        # maskFeatures = y

        maskActiv1 = self.activationFuncG_A(maskFeatures)
        maskActiv = self.conv1(maskActiv1)
        maskUpdate = self.updateMask(maskFeatures)

        return convOut, maskUpdate, convFeatures, maskActiv

#2. Code for the Muti-feature fusion block
class Muti_feature_fusion_block(nn.Module):
    def __init__(self, dim, drop_rate=0., layer_scale_init_value=1e-6):
        super().__init__()
        self.dwconv = nn.Conv2d(dim, dim, kernel_size=5, padding=2, groups=dim)  # depthwise conv
        self.norm = LayerNorm(dim * 2, eps=1e-6, data_format="channels_last")
        self.pwconv1 = nn.Linear(dim * 2, 4 * dim)  # pointwise/1x1 convs, implemented with linear layers
        self.act = nn.GELU()
        self.pwconv2 = nn.Linear(4 * dim, dim)
        self.gamma = nn.Parameter(layer_scale_init_value * torch.ones((dim,)),
                                  requires_grad=True) if layer_scale_init_value > 0 else None

        self.dwconv_mask = nn.Conv2d(dim, dim, kernel_size=5, padding=2, groups=dim)  # depthwise conv
        self.conv1_cat = nn.Conv2d(dim, dim, kernel_size=1, padding=0, bias=False)

    def forward(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:
        shortcut = x
        x = self.dwconv(x) # image feature information

        y = self.dwconv_mask(y) # PSF faeture information
        y = self.conv1_cat(y)

        x = torch.cat((x, y), 1)
        x = x.permute(0, 2, 3, 1)  # [N, C, H, W] -> [N, H, W, C]
        x = self.norm(x)
        x = self.pwconv1(x)
        x = self.act(x)
        x = self.pwconv2(x)
        if self.gamma is not None:
            x = self.gamma * x
        x = x.permute(0, 3, 1, 2)  # [N, H, W, C] -> [N, C, H, W]
        x = shortcut + x
        return x

#3. Code for the Frequency domain self-attention block
class Frequency_domain_self_attention_block(nn.Module):
    def __init__(self, dim, drop_rate=0., layer_scale_init_value=1e-6):
        super().__init__()
        self.dwconv_k = nn.Conv2d(dim * 2, dim * 2, kernel_size=7, padding=3, groups=dim)
        self.dwconv_q = nn.Conv2d(dim * 2, dim * 2, kernel_size=7, padding=3, groups=dim)
        self.dwconv_v = nn.Conv2d(dim * 2, dim * 2, kernel_size=7, padding=3, groups=dim)  # depthwise conv
        self.conv1 = nn.Conv2d(dim, dim * 2, kernel_size=1, bias=False)

        ## A version with reduced computational cost.
        # self.dwconv_k = nn.Conv2d(dim, dim, kernel_size=7, padding=3, groups=dim)
        # self.dwconv_q = nn.Conv2d(dim, dim, kernel_size=7, padding=3, groups=dim)
        # self.dwconv_v = nn.Conv2d(dim, dim, kernel_size=7, padding=3, groups=dim)
        # self.conv1 = nn.Conv2d(dim, dim, kernel_size=1, bias=False)
        # self.norm_kv = LayerNorm(dim, eps=1e-6, data_format="channels_last")
        # self.norm = LayerNorm(dim, eps=1e-6, data_format="channels_last")

        # self.pwconv1 = nn.Linear(dim, 4 * dim) 

        self.norm_kv = LayerNorm(dim * 2, eps=1e-6, data_format="channels_last")
        self.norm = LayerNorm(dim * 2, eps=1e-6, data_format="channels_last")

        self.pwconv1 = nn.Linear(dim * 2, 4 * dim)  # pointwise/1x1 convs, implemented with linear layers
        self.act = nn.GELU()
        self.pwconv2 = nn.Linear(4 * dim, dim)
        self.gamma = nn.Parameter(layer_scale_init_value * torch.ones((dim,)),
                                  requires_grad=True) if layer_scale_init_value > 0 else None

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        shortcut = x
        x = self.conv1(x)
        v = self.dwconv_v(x)
        k = self.dwconv_k(x)
        q = self.dwconv_q(x)

        q_fft = torch.fft.rfft2(q.float())
        k_fft = torch.fft.rfft2(k.float())
        out_kv = q_fft * k_fft
        out = torch.fft.irfft2(out_kv)
        out_size = out.size(-1)
        out = torch.roll(out, (out_size // 2, out_size // 2), dims=(0, 1))

        out = out.permute(0, 2, 3, 1)
        out = self.norm_kv(out)
        out = out.permute(0, 3, 1, 2)

        x = v * out
        x = x.permute(0, 2, 3, 1)  # [N, C, H, W] -> [N, H, W, C]
        x = self.norm(x)
        x = self.pwconv1(x)
        x = self.act(x)
        x = self.pwconv2(x)
        if self.gamma is not None:
            x = self.gamma * x
        x = x.permute(0, 3, 1, 2)  # [N, H, W, C] -> [N, C, H, W]
        x = shortcut + x

        return x

#4. I put the code of Convnext here help you understand these code, which are from the paper "A ConvNet for the 2020s".
class ConvNeXt_Block(nn.Module):
    r""" ConvNeXt Block. There are two equivalent implementations:
    (1) DwConv -> LayerNorm (channels_first) -> 1x1 Conv -> GELU -> 1x1 Conv; all in (N, C, H, W)
    (2) DwConv -> Permute to (N, H, W, C); LayerNorm (channels_last) -> Linear -> GELU -> Linear; Permute back
    We use (2) as we find it slightly faster in PyTorch

    Args:
        dim (int): Number of input channels.
        drop_rate (float): Stochastic depth rate. Default: 0.0
        layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.
    """

    def __init__(self, dim, drop_rate=0., layer_scale_init_value=1e-6):
        super().__init__()
        self.dwconv = nn.Conv2d(dim, dim, kernel_size=7, padding=3, groups=dim)  # depthwise conv
        self.norm = LayerNorm(dim, eps=1e-6, data_format="channels_last")
        self.pwconv1 = nn.Linear(dim, 4 * dim)  # pointwise/1x1 convs, implemented with linear layers
        self.act = nn.GELU()
        self.pwconv2 = nn.Linear(4 * dim, dim)
        self.gamma = nn.Parameter(layer_scale_init_value * torch.ones((dim,)),
                                  requires_grad=True) if layer_scale_init_value > 0 else None

        self.drop_path = DropPath(drop_rate) if drop_rate > 0. else nn.Identity()
        #self.apply(self._init_weights)

    def _init_weights(self, m):
        if isinstance(m, (nn.Conv2d, nn.Linear)):
            nn.init.trunc_normal_(m.weight, std=0.2)
            nn.init.constant_(m.bias, 0)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        shortcut = x
        x = self.dwconv(x)
        x = x.permute(0, 2, 3, 1)  # [N, C, H, W] -> [N, H, W, C]
        x = self.norm(x)
        x = self.pwconv1(x)
        x = self.act(x)
        x = self.pwconv2(x)
        if self.gamma is not None:
            x = self.gamma * x
        x = x.permute(0, 3, 1, 2)  # [N, H, W, C] -> [N, C, H, W]

        x = shortcut + self.drop_path(x)
        return x

def drop_path(x, drop_prob: float = 0., training: bool = False):
    """Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).

    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,
    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...
    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for
    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use
    'survival rate' as the argument.

    """
    if drop_prob == 0. or not training:
        return x
    keep_prob = 1 - drop_prob
    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets
    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)
    random_tensor.floor_()  # binarize
    output = x.div(keep_prob) * random_tensor
    return output


class DropPath(nn.Module):
    """Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).
    """

    def __init__(self, drop_prob=None):
        super(DropPath, self).__init__()
        self.drop_prob = drop_prob

    def forward(self, x):
        return drop_path(x, self.drop_prob, self.training)


class LayerNorm(nn.Module):
    r""" LayerNorm that supports two data formats: channels_last (default) or channels_first.
    The ordering of the dimensions in the inputs. channels_last corresponds to inputs with
    shape (batch_size, height, width, channels) while channels_first corresponds to inputs
    with shape (batch_size, channels, height, width).
    """

    def __init__(self, normalized_shape, eps=1e-6, data_format="channels_last"):
        super().__init__()
        self.weight = nn.Parameter(torch.ones(normalized_shape), requires_grad=True)  # gamma
        self.bias = nn.Parameter(torch.zeros(normalized_shape), requires_grad=True)  # beta
        self.eps = eps
        self.data_format = data_format
        if self.data_format not in ["channels_last", "channels_first"]:
            raise ValueError(f"not support data format '{self.data_format}'")
        self.normalized_shape = (normalized_shape,)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        if self.data_format == "channels_last":
            return F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)
        elif self.data_format == "channels_first":
            # [batch_size, channels, height, width]
            mean = x.mean(1, keepdim=True)
            var = (x - mean).pow(2).mean(1, keepdim=True)
            x = (x - mean) / torch.sqrt(var + self.eps)
            x = self.weight[:, None, None] * x + self.bias[:, None, None]
            return x

%5 Part of loss function need the VGG16.
class VGG16FeatureExtractor(nn.Module):
    def __init__(self):
        super(VGG16FeatureExtractor, self).__init__()
        vgg16 = models.vgg16(pretrained=False)  # False
        vgg16.load_state_dict(torch.load('F:/vgg16-397923af.pth'))
        self.enc_1 = nn.Sequential(*vgg16.features[:5])
        self.enc_2 = nn.Sequential(*vgg16.features[5:10])
        self.enc_3 = nn.Sequential(*vgg16.features[10:17])

        # fix the encoder
        for i in range(3):
            for param in getattr(self, 'enc_{:d}'.format(i + 1)).parameters():
                param.requires_grad = False

    def forward(self, image):
        results = [image]
        for i in range(3):
            func = getattr(self, 'enc_{:d}'.format(i + 1))
            results.append(func(results[-1]))
        return results[1:]

%6 add used module 
class MaskUpdate(nn.Module):
    def __init__(self, alpha):
        super(MaskUpdate, self).__init__()

        self.updateFunc = nn.ReLU(True)
        #self.alpha = Parameter(torch.tensor(alpha, dtype=torch.float32))
        self.alpha = alpha
    def forward(self, inputMaskMap):
        """ self.alpha.data = torch.clamp(self.alpha.data, 0.6, 0.8)
        print(self.alpha) """

        return torch.pow(self.updateFunc(inputMaskMap), self.alpha)
